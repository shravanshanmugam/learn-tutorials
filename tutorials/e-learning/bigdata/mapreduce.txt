Map Reduce
----------

Processing framework (batch/massive/parallel/distributed)

Advantages
----------

Cluster monitoring
Resource allocation
Cluster management
Scheduling
Execution
Speculative execution

Aim - achieve data locality
Identify location where block of data is stored and execute task on that location

Abstraction of Map Reduce
-------------------------

Hive, Pig, Sqoop, Oozie

Alternate of Map Reduce
-----------------------

Spark

Daemons in Map Reduce
---------------------

Job tracker and Task tracker - H1
Resource manager and Node manager - H2

Map transformation - parallelize
Reduce transformation - aggregate

Mapper
------

Input - Blocks, HDFS, File system, RDBMS, NoSQL etc.

Number of Mappers = Number of blocks = Size of data/Size of 1 block

Reducer
-------

Input - output of Mapper
Output - HDFS, File system, RDBMS, NoSQL etc.

Number of reducers >= 1

Development
-----------

Mapper class and Reducer class - job on some file
Create jar file
Send request to daemon Job tracker.
Job tracker requests location of block from Name node.
Name node sends metadata of file as response.
Job tracker sends mapper task request in parallel to task tracker of slave nodes [name node + task tracker] where block is present.
Task tracker launch Map JVM process. It reads data from Data node and start mapper process.
Output of mapper (a.k.a. intermediate data) is stored in local file system (ext file system in Linux) of that node.
Task tracker sends heartbeat to job tracker (every 3 seconds).
Job tracker will come to know once mapper task is completed.
Once all mapper tasks are completed, job tracker will start the reducer task.
Reducer task will execute in one of the mapper nodes or any other free node.
Task tracker launch Reduce JVM process.
Reducer process requests output of Mapper from mapper nodes. Output is transferred to Reducer node via HTTP.
Once reducer is completed, output is store in local file system and then goes to HDFS.
Task tracker sends heartbeat to job tracker.
Job tracker will come to know once reducer task is completed.

What happens When two blocks of data are present on same node?

When task tracker gets request from Job tracker it starts two Mapper JVM and two Reducer JVM process.
One task for each block. If there is only one block of data, the second process will be idle.
Default value is 2.

Data formats
------------

Data input/output for Mapper and Reducer is key-value pair.
1. Text/text format
Key - offset of the record (not line number. offset will wrap around to next line)
Value - Record itself

What happens When three blocks of data are present on same node?

Third block task goes to queue and executes when one of the block task is completed.
Or we can change the configuration of number of Mapper JVM processes.

Handling failure
----------------

If any node goes down when executing a task, Job tracker will get replica information of the block and re-assign task to second node.

Map reduce example
------------------

Sales data

id,platform,product,amount
B0
1,amazon,mobile,2000
2,amazon,tv,1000
3,flipkart,ac,3000
B1
4,flipkart,washingmachine,5000
5,amazon,cycle,5500
6,ebay,watch,7800
7,flipkart,laptop,9000

Query
-----

select platform, sum(amount) from sales group by platform;

Design
------

Input -> B0, B1 -> Map -> Shuffle [sort by key and group by key] -> Reduce
Shuffle happens internally in Map Reduce

Input ->
K1  V1
0   1,amazon,mobile,2000
31  2,amazon,tv,1000
56  3,flipkart,ac,3000

78  4,flipkart,washingmachine,5000
95  5,amazon,cycle,5500
121 6,ebay,watch,7800
143 7,flipkart,laptop,9000

Map ->
K2 V2
amazon 2000
amazon 1000
flipkart 3000

flipkart 5000
amazon 5500
ebay 7800
flipkart 9000

Shuffle ->
K3 V3
amazon [2000, 1000, 5500]
ebay [7800]
flipkart [3000, 5000, 9000]

Reduce ->
amazon 8500
ebay 7800
flipkart 17000

Map Reduce data type
--------------------

Java - int, float, String, boolean
MR - IntWritable, FloatWritable, Text, BooleanWritable - serialized data types

Skeleton code
-------------

Main class -> 2 inner classes - one for mapper, one for reducer and one main method for driver code

Mapper class extends Mapper<InputKey, InputValue, OutputKey, OutputValue>
Reducer class extends Reducer<InputKey, InputValue, OutputKey, OutputValue>

methods
1. setup - one time for block
2. map/reduce - one time for each row
3. run - driver related code
4. cleanup

Input split
-----------

Block - physical
Split - logical unit of block

Problem - block can get split unevenly. meaning block of data doesn't end exactly at last column of a row.

Split will logically connect the row that got separated by physical blocks

1 Block = 1 Split = 1 Mapper
1 Block <> 1 Split = 1 Mapper

We can increase or decrease number of mapper tasks with help of split config.

Speculative execution
---------------------

In case one of the task tracker gets stuck, Job tracker will assign mapper job to new task tracker on a new node which has same block of data.
This creates duplicate job. Original task is not killed. Job tracker will take result of whichever task completes first.