Hive
----

Invented by Facebook.
Hive is a Query Engine. Not a database.
Uses SQL to communicate with Map Reduce instead of Java code.
Abstraction of Map Reduce.

Install hive.
CLI and Hive server.

Hive server stores metadata of table - columns, indexes etc.
Hive metadata is stored in RDBMS. RDBMS is required for Hive.
This is also known as remote meta store.
Hive does not store the information itself. Data is stored in HDFS itself.
Insert/load data is stored in HDFS.
Read queries on table will look up metadata information and then retrieve data from HDFS and output result in table form.

If RDBMS is not installed for Hive, Hive will store metadata in Embedded RDBMS called Derby.

Why not Embedded meta store?
Embedded data store is local to hive server. In a cluster of hive nodes, the meta data will not be shared.
For example create table request will execute on one node and read request will execute on another node which will not have the table itself.

Hive SQL
--------

create database.
create table.
describe table.
describe extended table.
show functions. (pre-defined hive functions)
select query with where clause.
map reduce job gets triggered only when executing aggregate queries.

Loading data
------------

1. row format delimited fields by ',' line terminated by '\n'. store as text file. load data into table from file path (local or hdfs).

2. hive files are stored in /user/hive/warehouse by default.
all databases and tables are created in this directory.
we can directly put files into hdfs hive directory to create a table in database.

3. insert query into table.

Internal and External tables
----------------------------

Dropping internal table will delete hive tables and HDFS data.
Dropping external table will delete hive tables but not delete HDFS data.

ACID tables should be Internal tables.
Creating external table will not delete HDFS data when hive table is dropped.

Hive Partition
--------------

Static (default) partition
1. partition column defined in schema
2. insert query by partition column and value
Dynamic partition
1. switch dynamic partition mode
2. insert query by partition column and without value

Cannot load data into a partitioned table. We can only insert with query.
Map Reduce job will be triggered for insert query to place record in correct partition.
We can insert into a partition table by reading from non partition table.

HDFS creates a folder for partition for each value.
We can view partitions of table.

Commonly used partition - Year, sub partition by month, sub partition by day.

Hive bucket
-----------

1. Data sampling
2. Map side join

Bucket is used to distribute the data.
switch dynamic partition mode and bucketing

Similar to partition, cannot load data to bucketing table. We can only insert with query or using existing non bucketing table.
Create bucketing by Table schema clustered by column name into number of buckets.
Hive decides which bucket to place a record using Hash partition (hash of bucket column mod number of buckets)
HDFS creates n files one for each bucket.

Why bucketing instead of partition?
Partitioning is required when values repeat in columns so we can group by them.
Bucketing can be used when we need to partition by columns with unique values i.e. partitioning is not possible but we also want to avoid full scan.
Query by bucketing column will query directly on the correct partition which avoids full scan.


Data sampling
-------------

1. Block
2. Bucketed

Bucket count
------------

1. Table size - based on dry run/historical data
2. Block size

Empirical formula for bucket count.
Bucket count = 2^n >= Table size/Block size
E.g.
2300MB/128MB = 17.96
2^n >= 17.96 = 32

Based on this formula, we only need to change bucket count when table size (i.e. data) doubles.

Changing bucket count requires repartitioning.
Create new table with new bucket count.
Insert overwrite data into new table selecting from old table.
And lastly drop old table.
Rename new table as old table.

File format
-----------

Text file
RCFile [14% smaller]
Parquet [62% smaller]
ORCFile [78% smaller]

Processing time is reduced when using compressed file formats.
Define in table scheme store file format.
ORCFile - Column oriented storage. Aggregation will be faster.
Data can only be inserted using insert query or insert overwrite query selecting from another table. We cannot load data from file.

ACID Table
----------

Supports Insert, update and delete records.

Need to set few hive properties to enable ACID Table. We can set the properties in a hql file where we have our schema creation queries.

1. Has to be internal table
2. Has to be bucketed
3. Must use ORCFile format
4. Must enable table property transactional=true.

Each insert, update, delete query will trigger Map Reduce job.

User Defined Functions
----------------------

Create new Java project.
Create a class extends UDF. Implement evaluate method.
Can take multiple columns as input and return Text output.
Build a jar of this project.
Copy jar to hive server.
Add jar to hive shell.
Create temporary function as package.ClassName

When exiting hive shell, function will be deleted. We need to add this function whenever we start hive shell. We can add along with our schema hql file.

For example, custom hashing function, convert data to new data format.